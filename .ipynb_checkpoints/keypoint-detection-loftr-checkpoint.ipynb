{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e4d5bb",
   "metadata": {},
   "source": [
    "### Import image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7820b",
   "metadata": {},
   "source": [
    "### Scale Space Extrema Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d1a068",
   "metadata": {},
   "source": [
    "#### Generate Image Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b749011",
   "metadata": {},
   "source": [
    "#### Calculate Difference of Gradients (DoG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392b0cf",
   "metadata": {},
   "source": [
    "Calculate DoG by subtracting adjacent images within the same octave\n",
    "![DoG](https://miro.medium.com/v2/resize:fit:828/format:webp/0*DlULvyAuyXb1mSWb.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76d10c",
   "metadata": {},
   "source": [
    "The **DoG** variable now is a 2D list (list of lists). Each element in the list is an Octave (list). And each element in the octave list is a DoG Image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b15bc9",
   "metadata": {},
   "source": [
    "#### Keypoint Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f379072",
   "metadata": {},
   "source": [
    "To perform Keypoint Localization, need to define our region of interest. The region of interest, based on the figure below, for any given point are the 9 points above and below, and 8 points surrounding our point of interest, if any. \n",
    "![](https://miro.medium.com/v2/resize:fit:466/format:webp/0*nbK933cOIyNrmhWi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1295dd7",
   "metadata": {},
   "source": [
    "#### Keypoint Localization for ONE DoG layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b0cf6",
   "metadata": {},
   "source": [
    "#### Filter Keypoints on ONE DoG Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38956930",
   "metadata": {},
   "source": [
    "Keypoints on FLAT regions have a corresponding low magnitude harris corner response |R| \n",
    "\n",
    "Keypoints on EDGE regions have a corresponding negative magnitude harris corner response R\n",
    "\n",
    "To reject these keypoints, we define a positive threshold value, and only accept keypoints that corresponds to values higher than the threshold value.\n",
    "\n",
    "(ref: https://docs.opencv.org/3.4/dc/d0d/tutorial_py_features_harris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac58f84",
   "metadata": {},
   "source": [
    "#### Keypoint Localization (Across All DoG Layers + Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6385b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3861a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "loftr = KF.LoFTR(pretrained='outdoor').to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def detect_and_match_loftr(img1, img2):\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    tensor1 = K.image_to_tensor(img1_gray, False).float() / 255.0\n",
    "    tensor2 = K.image_to_tensor(img2_gray, False).float() / 255.0\n",
    "\n",
    "    input_dict = {\n",
    "        \"image0\": tensor1.to(device),\n",
    "        \"image1\": tensor2.to(device)\n",
    "    }\n",
    "\n",
    "    out = loftr(input_dict)\n",
    "    mkpts0 = out[\"keypoints0\"].cpu().numpy()\n",
    "    mkpts1 = out[\"keypoints1\"].cpu().numpy()\n",
    "\n",
    "    return img1, img2, mkpts0, mkpts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(img1, img2, mkpts0, mkpts1):\n",
    "    matched_img = cv2.drawMatches(\n",
    "        img1, [cv2.KeyPoint(x=float(x), y=float(y), _size=1) for x, y in mkpts0],\n",
    "        img2, [cv2.KeyPoint(x=float(x), y=float(y), _size=1) for x, y in mkpts1],\n",
    "        [cv2.DMatch(_queryIdx=i, _trainIdx=i, _imgIdx=0, _distance=0) for i in range(len(mkpts0))],\n",
    "        None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(cv2.cvtColor(matched_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"LoFTR Keypoint Matches\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread(\"image1.jpg\")  # Replace with your image paths\n",
    "img2 = cv2.imread(\"image2.jpg\")\n",
    "\n",
    "img1, img2, mkpts0, mkpts1 = detect_and_match_loftr(img1, img2)\n",
    "visualize_matches(img1, img2, mkpts0, mkpts1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
